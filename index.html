<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>面向前端融合的编码摄像理论与技术</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }
        header {
            background: #002a7e;
            color: white;
            /* padding: 30px 0; */
            text-align: center;
            height: 100%;
        }
        h1, h2, h3 {
            color: #002a7e;
        }
        h1 {
            margin-top: 0;
        }
        section {
            margin-bottom: 40px;
            width: 90%;
        }
        section h2 {
            border-bottom: 2px solid #002a7e;
            padding-bottom: 10px;
        }
        section h3 {
            margin-top: 20px;
        }
        p {
            margin: 0% 0;
            font-size: 20px;
            text-indent: 2em;
        }
        footer {
            text-align: center;
            padding: 20px 0;
            background: #333;
            color: white;
            margin-top: 40px;
        }
        img {
            max-width: 90%;
            /* height: auto;  */
            display: block;
            margin: 20px auto;
        }
        .container {
            width: 100%;
            margin: 20px auto;
            max-width: 1200px; 
        }
        li {
            font-size: 20px;
        }
    </style>
</head>
<body>
    <header>
        <p class="container">
            <img style="max-width: 80%; margin: 5%;" src="images/logo.png" alt="logo">
        </p>
    </header>
    <div class="container">
        <section id="introduction">
            <h2>项目介绍</h2>
            <p>传统成像技术普遍采用“先采集，后处理”的模式，但在实际应用中面临采集速度受限、感知灵敏度不足、实时性差等多重挑战。其根本原因在于前端光学采集系统与后端计算方法的割裂，导致光信息采集通量受限、成像质量不佳及能耗水平较高。为此，本项目提出了前端光学采集与后端计算深度融合的新型编码成像体系，旨在突破传统成像技术的性能瓶颈与能耗限制。项目围绕高速、高灵敏度及低功耗成像目标，开展了高时空分辨率、高灵敏度、低功耗编码摄像理论与关键技术研究，继而设计轻量级原型系统，提高了编码摄像系统的实用性。在上述研究的基础上，项目组设计并搭建了面向无人系统应用的编码成像验证平台，通过应用验证与示范，推动技术落地，为无人驾驶、智能监控及生物医学成像等领域提供理论支持与技术保障。</p>
            <img src="images/项目简介.png" alt="项目简介">
        </section>

        <section id="achievements">
            <h2>代表性研究成果</h2>
            <!-- <ol>
                <li>针对高速运动场景成像过程中时间分辨率和空间分辨率无法兼顾的难题，提出了强度-事件联合的混合编码成像方法、复合编码孔径压缩成像方法、深度光学调制的压缩成像方法等，解决了高速高分辨成像面临的带宽限制问题，能够同时捕获高动态场景空间结构和时域变化细节。</li>
                <li>针对弱光环境中噪声水平高、动态范围大、语义感知困难的问题，提出了亚像素级配准的弱光-强光双路采集系统、构建了有标注的大规模视频数据库，提出了监督式时-空-谱联合的弱光视频增强方法，并设计了低照度视频中光照自适应的物体检测方法， 为极低光照条件下的高质量成像和分析提供了可靠技术手段。</li>
                <li>针对移动平台载荷受限和计算资源不足的问题，设计与搭建了轻型的编码曝光采集方法和装置、便携式宽视场高分辨光谱采集方法和装置等，提出了基于模型-数据联合驱动的高效重建框架和神经网络多项式展开技术，显著降低了编码成像系统的体积重量和解码重建算法功耗，为资源受限平台的视觉应用提供了有力支持。</li>
                <li>项目开发的编码摄像感知方法应用于自动驾驶、生物医学和工业检测等领域，为无人物流车、服务机器人、基因测序设备、病理切片扫描仪器、高速工业产线缺陷检测等系统提供装备与技术支撑，稳定运行于多类应用场景，取得了显著的经济效益和社会效益。</li>
            </ol>	 -->
                <h3>理论研究成果</h3>
                    <ol>
                        <li>神经网络因其万能近似特性被广泛应用于社会各个领域，但也因其黑箱特性缺乏解释性，并且计算复杂度较高，限制了其推广应用。本研究提出HOPE框架，对神经网络进行高阶多项式展开，从而实现黑箱的透明化。研究首先推导了复合函数的高阶求导法则，并将其应用于深度神经网络，实现了神经网络高阶导数的快速计算，并得到了神经网络的高阶Taylor多项式。Taylor展开对神经网络进行局部解释，当所有局部解释达到一致结论时，便提供了全局解释。本研究通过对Taylor近似精度、运行效率、收敛性、误差边界等进行评估，验证了方法的高效性，并将其应用于函数发现、快速推理、特征挑选。HOPE 研究了神经网络本质问题，作为一种基础理论，其在网络高阶优化、网络轻量化、网络结构设计等方面也有潜在应用。相关研究成果发表于领域顶级期刊 IEEE Transactions on Pattern Analysis and Machine Intelligence。</li>
                        <li>项目提出的低照度环境监督式数据增强方法在微观成像中也具有普适性和重要意义。针对极低光子条件下的高质量成像及单分子FRET测量问题，提出一种被称为“多帧双通道融合去噪网络（MUFFLE）” 深度学习的信号重建策略，其结构包括bi-ConvLSTM、U-net CNN和Dual-chanel Residual Fusion 模块，充分利用单分子FRET视频数据在时间、空间和光谱层面的冗余信息，通过对视频弱信号进行增强和去噪，实现了在每帧仅需60-70个光子的极弱单分子荧光条件下，进行长时程的单分子FRET测量。该研究可作为低光子单分子荧光测量的一般处理步骤，与其他直接增强荧光团光稳定性的化学技术一起，促进生物分子动力学的长期精确记录。相关成果发表在领域顶级期刊 Nature Communications。</li>
                    </ol>
                <h3>应用研究成果</h3>
                    <ol>
                        <li>针对多光谱成像系统难以在保持小型化设计的同时实现超过百万像素的高分辨率的问题，提出了一种手持式多光谱视频相机系统，能够在可见光范围内实现12个波段、6500万像素的高分辨多光谱视频成像。该系统通过结合轻薄胶片掩模、光纤面板技术和基于深度学习的解码算法，解决了当前多光谱成 像设备在小型化和高分辨率之间的矛盾。这一创新设计有效克服了传统快照光谱成像系统因体积庞大、成本高昂以及计算耗时等带来的限制。通过多个场景的成像实验，该系统展示了在农业、水文等领域的快速成像能力和精细化数据捕捉优势，尤其适合户外宏观场景的高分辨率监测。未来，这种技术有望与便携式主机集成，应用于低载荷无人机和移动机器人等设备，推动大范围场景的精细化环境监测。相关成果发表在领域顶级期刊 Nature Communications。</li>
                        <li>在基于编码曝光的高速高分辨轻型编码摄像方面，本研究开发了一种新颖的模糊分解框架，将编码曝光的前向成像模型与自然视频的隐式神经表征相结合，实现了高分辨率、低带宽的轻量级高速摄影。该框架通过在数据采集过程中将运动方向线索隐式嵌入编码模糊图像，巧妙地应对了运动方向模糊的挑战。此外，该框架可以灵活地扩展到模糊视频的时间超分辨率和运动去模糊，而无需进行任何修改或重新训练。与现有的模糊分解方法相比，该框架具有系统复杂度低、模型体积小、应用灵活性高等优点。该成果为低带宽、低成本、高速成像开辟了一条前景广阔的道路，并为视频监控、视频助理裁判、自动驾驶、检测等移动视觉系统的应用带来新的启示。相关研究成果发表于领域顶级期刊 International Journal of Computer Vision。</li>
                        <li>针对高动态场景下的快照压缩成像，提出一种新颖的混合“强度+事件”成像方案，将事件相机纳入视频 SCI 设置中。所提出的系统由双路径光学装置组成，可同时记录编码的强度测量和中间事件信号。同时，开发了一个双分支 Transformer，利用两种数据模式之间的内在关系来解码密集的视频帧。对模拟和真实数据进行的大量实验证明了我们优于最先进的视频 SCI 和视频帧插值 (VFI) 方法。得益于新的混合设计，本研究设计的事件摄像机可以采用低成本 CMOS 图像传感器以 24 FPS 的速度工作，同时实现 0.1ms 时间间隔的高质量视频拍摄。这项研究不仅推动了计算成像技术的发展，还为动态场景捕捉和重建的应用开辟了新途径，预示着高速摄像技术在各个领域的广泛前景。相关研究成果发表于领域顶级期刊 IEEE Transactions on Pattern Analysis and Machine Intelligence。</li>
                        <li>针对千万像素级高速高分辨成像系统的小型化难题，本项目结合衍射光学和压缩感知理论，开发了端到端的高速高分辨压缩成像系统。为了满足低带宽高速高分辨视频的采集和重建，我们引入点扩展函数设计来编码场景精细的空间结构和时域压缩成像来编码场景的快速运动，借助深度神经网络的性能优势和端到端的计算特性，联合优化了衍射光学元件的参数和后端重建模块。该方法成像效果优于传统的分步成像和重建方法，仅采用低带宽的工业级传感器（五百万像素，23.8fps）实现了8倍的时间分辨率提升（190fps）和 4x4倍 的空间分辨率提升（八千万像素），总共128倍数据通量提升。在上述框架下，我们搭建了小型化的高速高分辨成像原型样机进行原理验证，为产品开发奠定了基础。相关研究成果投稿于领域顶级期刊 OSA Optica。</li>
                        <li>针对低照度环境下的目标检测器存在成像质量与运动模糊难以权衡的问题，本研究设计了一种可编程快门系统，通过该系统构建了一个用于编码曝光的原型，从而实现有效的编码。解码方面，设计了一个端到端的深度学习网络，称为编码快照检测（DECENT），它能够从动态场景的编码模糊快照中顺序地检索出检测框。为了提升网络的学习效率，本研究将相机噪声引入编码曝光的成像模型，生成了近似真实的数据，避免了训练过程中因数据不真实而造成的问题。这种方法在真实的夜间视频中展示了良好的泛化能力，尤其在低照度环境下表现突出。该方法具有低带宽、低成本、紧凑且高精度的优势，为夜间监控提供了一种可行的解决方案。相关研究成果发表于图像处理领域一区 SCI 期刊 IEEE Transactions on Image Processing。</li>
                        <li>无人机夜间高动态范围环境感知方面，针对夜间场景动态范围大、不同帧之间的光照变化突然、欠曝导致某些时刻无法跟踪识别关键物体等问题，我们利用邻近视频帧的时域连续性和一致性，引入事件相机辅助，充分利用事件感知对于大动态范围和低照度场景的鲁棒性捕获场景结构，继而引导用RGB视频中高质量对应物补偿低质量图像，实现持续高质量的RGB视频采集以及鲁棒的下游任务。该方法为无人机在复杂光照条件下的环境感知和目标跟踪提供了一种创新的解决方案。通过结合事件相机和RGB相机的优势，我们提出的事件辅助目标跟踪算法能够在高动态范围场景中实现更准确和鲁棒的目标跟踪，这对于无人机在多种实际应用中的性能至关重要，如搜索救援、监控和农业监测等。这项研究不仅提高了无人机在极端照明条件下的作业能力，还为未来无人机技术的进一步发展和应用奠定了基础，尤其是在自动化和智能化方面。相关工作发表于领域旗舰期刊 Drones。</li>
                    </ol>
        <section id="application">
            <h2>推广应用</h2>
            <h3>应用案例一：移动平台视觉导航</h3> 
                <p>基于项目开发的面向前端融合的编码摄像关键技术，团队先后与东风悦享科技有限公司、北京云迹科技股份有限公司、驭道无驾科技有限公司、中科原动力科技有限公司、天津大学等单位合作，研制了卡车、网约车、巴士、拖拉机等无人智能交互系统9套，目前已应用于东风Sharing-VAN、中通公交车、中国重汽豪沃TX/T5G、云迹智能机器人等10余种无人车和服务机器人，稳定运行于城市园区、国际港口、全国各大酒店和大田种植等多类应用场景，取得了显著的经济效益和社会效益，为无人驾驶商业应用落地做出了突出贡献。。</p>
                <img src="images/推广1.png" alt="推广1">
            <h3>应用案例二：高速高分辨工业检测系统</h3> 
                <p>项目研发的高速高分辨编码摄像关键技术在生物医学和工业检测领域取得了推广和应用，为基因测序设备、病理切片扫描仪器、高速工业产线缺陷检测系统提供装备与技术支撑。研发装置在新冠疫情期间服务快速核酸检测和病毒测序、 医疗器材与药品包装印刷质量检测，服务健康中国战略；提升了新能源组件高速产线质量监测，推动企业智能化转型升级，降低资源浪费，为“双碳”战略提供技术支撑，并取得了显著经济和社会效益。</p>
                <img src="images/推广2.png" alt="推广2">
        </section>

        <section id="summary">
            <h2>成果总结</h2>
                <!-- <p>课题组共发表学术论文47篇，包含Nature Communications、IEEE TPAMI、Springer IJCV、OSA Optica、Proc. IEEE、PNAS等领域顶级期刊及CVPR、ICCV、AAAI、ICLR等顶级国际会议；获省部级科技奖励一等奖3项、二等奖1项、科技竞赛一等奖1项；获授权国家发明专利授权13项，并有10项处于实审阶段；登记软件著作权8项；培养博士生8名（其中3名获高校教职）、硕士生10名（1名获评江苏省优秀硕士学位论文）、博士后出站2名，项目成员1名获得国家级青年人才称号。</p> -->
                <P>学术交流方面，项目成员作为会议主席组织和承办国际学术会议Photonics Asia---Optoelectronic Imaging and Multimedia Technology XI; 作为Area Chair参与组织国际会议ISAIR 2020~2022；作为程序委员参与组织国际会议，包括 ICME 2020~2023、Photonics Asia 2021-2023、ISPA 2020 等; 组织国内/国际学术会议/论坛20余次，其中2个论坛获最佳组织奖，包括VRHCIAI 2022---Intelligent Interaction Technology workshop和ICIVIS 2022---Intellisensor and Cross-modal Recognition workshop; 作为客座编辑组织IEEE JSTSP的专刊“Selected Topics on Deep Learning for High-Dimensional Sensing”; 在多国际/国内学术会议上做学术报告和进行同行交流，包括CITA2021 2022、VALSE 2022、中国光谷人工智能大会暨企业家高峰论坛、光学青年科学家论坛、中国量子成像大会等。</P>
                <P>人才培养方面，毕业博士生8名（3位获高校教职，担任助理教授）、硕士生10名（江苏省优秀硕士论文奖1人次）; 项目负责人获聘教育部青年长江学者，并晋升长聘副教授；课题负责人晋升教授2名，骨干成员晋升副教授1名；出站博士后2名，均晋升副高职称。</P>
                <P>平台建设方面，搭建了一套前端融合视觉导航的无人车系统。通过关键技术选型、硬件集成、软件开发、系统测试与优化等步骤，可以实现室内场景SLAM建图、厘米级高精度定位、全局与局部路径规划和车体运动控制等功能。该系统目前可以作为一个通用的视觉导航测试与验证平台，为未来无人驾驶相关的感知与认识技术发展提供平台支撑，也可以作为相关领域研究生培养的教学实践平台。</P>   
            </section>
    </div>
    <footer>
        <p>&copy; 2025 THU-BUU-NUPT联合项目组。保留所有权利。</p>
    </footer>
</body>
</html>